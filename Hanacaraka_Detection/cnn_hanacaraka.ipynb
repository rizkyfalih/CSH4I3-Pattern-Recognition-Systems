{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_convolutional_net_hanacaraka.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "AHZ1M5GtBwtU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "09e07843-5589-4ba7-b271-b6834ca6bbd9"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "i6H04wdh5QfO"
      },
      "cell_type": "markdown",
      "source": [
        "# Import dependencies"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "guUyTDBLzseC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "27c3de68-aadd-40ad-9faa-cdb7666d1c34"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, AveragePooling2D, Dropout, SpatialDropout2D\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.preprocessing import image \n",
        "from keras import backend as K\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.applications.resnet50 import preprocess_input\n",
        "import PIL.ImageOps    \n",
        "from keras.utils import np_utils\n",
        "np.random.seed(23)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5OX6pgFZ5UPv"
      },
      "cell_type": "markdown",
      "source": [
        "# Load the Hanacaraka Dataset\n",
        "Dataset courtesy of Muhammad Soleh, HPC Lab 1231 Fasilkom UI"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-pnIt3fjzseN",
        "outputId": "22d28678-d274-4fbe-fe4f-ff34f3211a79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        }
      },
      "cell_type": "code",
      "source": [
        "folder = 'gdrive/My Drive/dataset/hanacaraka/'\n",
        "\n",
        "# Settings\n",
        "img_rows, img_cols = 60, 78         \n",
        "nb_classes = 20\n",
        "number_of_data = 2000\n",
        "\n",
        "X_train = []\n",
        "Y_train = []\n",
        "\n",
        "for cl, fld in enumerate(os.listdir(folder)):\n",
        "        for f in os.listdir(folder + fld):\n",
        "                img = image.load_img(folder + fld + '/' + f, grayscale=True, \n",
        "                                     target_size=(img_rows, img_cols))\n",
        "                img = PIL.ImageOps.invert(img)\n",
        "                img = image.img_to_array(img)\n",
        "                X_train.append(img.reshape((img_rows, img_cols, 1)))\n",
        "                Y_train.append(cl)\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "\n",
        "shuffled_indices = np.random.permutation(X_train.shape[0])\n",
        "X_train = X_train[shuffled_indices] \n",
        "Y_train = Y_train[shuffled_indices] \n",
        "X_test = X_train[X_train.shape[0]//2:]\n",
        "X_train = X_train[:X_train.shape[0]//2]\n",
        "\n",
        "Y_test = Y_train[Y_train.shape[0]//2:]\n",
        "Y_train = Y_train[:Y_train.shape[0]//2]\n",
        "\n",
        "print('X_train original shape:', X_train.shape)\n",
        "\n",
        "if K.image_dim_ordering() == 'th':\n",
        "    # For Theano backend\n",
        "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
        "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    # For TensorFlow backend\n",
        "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
        "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "# Split train test\n",
        "X_train = X_train.astype('float32') / 255.\n",
        "X_test = X_test.astype('float32') / 255.\n",
        "Y_train = np_utils.to_categorical(Y_train, nb_classes)\n",
        "Y_test = np_utils.to_categorical(Y_test, nb_classes)\n",
        "\n",
        "test_data = 1000\n",
        "\n",
        "X_train = X_train[:number_of_data]\n",
        "Y_train = Y_train[:number_of_data]\n",
        "X_test = X_test[:test_data]\n",
        "Y_test = Y_test[:test_data]\n",
        "\n",
        "print('X_train shape:', X_train.shape)\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "    \n",
        "\n",
        "# Visualize Hanacaraka dataset\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "n = 10  \n",
        "plt.figure(figsize=(10, 4))\n",
        "for i in range(n):\n",
        "    # display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(X_train[i].reshape(img_rows, img_cols))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train original shape: (5100, 60, 78, 1)\n",
            "X_train shape: (2000, 60, 78, 1)\n",
            "2000 train samples\n",
            "1000 test samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAAyCAYAAABYmgHDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJztXWl0VFW2/urWXKlUVZLKBCEJJJEw\nTwECqAwC4kMFRRTspSCt77VD62tRn7xufWL7pF2vbWdRERURASWCIKAoaCJDggEChDEh85xKqpJK\nzcN9P+I+3CqSEMgA4v3WqkVIqm7dc885++zz7W/vI+F5HiJEiBAhQoQIEdcyuCt9AyJEiBAhQoQI\nET0N0eERIUKECBEiRFzzEB0eESJEiBAhQsQ1D9HhESFChAgRIkRc8xAdHhEiRIgQIULENQ9ZR3+U\nSCS/6RQunuclF3vPb7WNEokEEokEPp/vqm0jx7X6036//4K/SSStt91RlmBn2/hb7UPCtTxOCddq\nG2mM8jwPv99/TbZRiIv142+1fb8Fe9pduFbnohDttbFDh0eEiK6AjMivhiTgb2I5BBEiRIgQ0ZsQ\nHR4RPQahkyOTyeD1esFxHDiOg9/vb5P5ESFChAgR3Qdi03vr2lfzZlZ0eER0iM4O6PYmFc/zkEgk\n8Hq9AMAcHWEoQIQIESJE9Ax62+FpC1eLnRcdHhE9htjYWAwZMgTR0dGQy+UoKCiAyWRCYWEhaR6u\n9C2KECFChIjfCUSH5zeKS/GY2xMICxkWpVIJiUQCp9OJkJAQSKVShIaGQiaTwWq1wmazwe/3g+d5\nxtaEhITA5/NBpVKxa7ndbqhUKsTGxuLGG2/E1KlTIZPJIJFI4HK5UFVVheXLl6OpqQk6nQ42m010\nfkR0CBpbUqkUPp8Pcrkcfr//Al0YgUKmnb2ucB4IPyuRSMBxXLvfI+LaQncwIcRcdzT+pFIpeJ5n\nY08ikUAqlcLr9QZ8TqFQgOd5eDyeLt8XoTvtLMdxCAkJgdVqRWRkJGw2G5xOJ3ieZ/NILpfD4/Gw\nuUuf6+g+aL2gdsvlcni93m5hiSQXyZK5Onioy8S1rkb/dWJdtI0cx/FA+6Eo4QRVKpWYNm0aJk6c\niJkzZ0Kj0aCurg6LFi2CyWSCy+VCfHw8BgwYgClTpmDQoEFQKBTQarXIyMhAQUEBFixYgLCwMJw+\nfRr79+9HdXU1OI7DXXfdhZEjR+L48eNYu3YtTpw4Abqv9jK5xCytVvxe2xgc9uQ4ji0WwRAaVeHP\nPQkxSysQv7X2CZ2PX1+X3YedCdG39R6pVAqgdaHneZ6NW5/Px5xu+n9XQNfyer3dMk7JcRE6MDKZ\njG1K3G43m4dSqZRJG+RyOSQSCdxu9wXXVCqV8Pv9zNlRKBTw+XyX3Pb2+lF0eARt7Ix3fjWhux0e\njUaD6dOn4+GHH0ZBQQFeeeUVJCYm4rHHHoPFYsGuXbswevRojBkzBk6nEzk5OTh27Bjq6+sxbNgw\nvPbaaygoKMDLL7+MnJwc1NXVBexiPB4P0tLSsGnTJmRkZGDp0qXM4IgOT8f4PbSRxmnQ5yCRSJjh\nk0qlcLlckEqlkMvl7HdOpxNAq4EUGtq2QGym2+0Gx3EBu0ng/Lij7+9M6QTR4WnFb7V93eEMCBf+\ni7EYQsdd6AQJ7XF3rUfB47m7xinNnf79+2PTpk2Qy+W45557kJ+fz75PKpUGJKgoFArGZLWnD6W5\n7XQ629zwdOZ5iGnpF4FwUPxeQIOJ53kolUqMHz8egwYNwhdffIEDBw6gqqoKEokE1dXVuOGGGxAd\nHQ29Xo/Dhw/j9OnT2L59OywWC3w+HwtznTt3DpmZmWhsbAQAlpXF8zxkMhlKS0shl8thNBqvZNNF\nXIVob/4JnROXywWVSgWPxwOn04k+ffpg8uTJ2LNnD2praztFe5ODTbtT+g6lUonQ0FDU19cHXEcU\n11/7uJSwelvjNDj02d6CTr8Thk1pkad7oLFGtlkul0Mul8Nut19yu4Lvt7vHsc/nw4wZM+D3+xER\nEYFRo0bhxIkTARtd4fe73W7IZDIolUq4XK4LrsdxHHN20tLScNddd6G+vh5r1qyByWTqsvMnOjy/\ngjqlsx7ktQCZTMZ2tjqdDnfccQfq6+vx+eefs/gxAGg0GoSHh+P555/HgQMHYLFYAAAqlYrtrHme\nR0NDAw4fPozGxkb2N6rB4/f7odPp4HQ6YbfbwfM8W7hEiGgPZDgjIiKQlJQEn8+HQ4cOQSqV4rnn\nnsPcuXMhlUoxevRorFq1CkVFRQA6pv9pjhuNRkRFReEvf/kLkpOTodFoYDKZ8Oijj6K5uTnAaRd1\nPNc2lEollEpllz7vcDgQExMDtVoNh8OBmpqaC95Heh23241+/fohPDwco0ePRkFBAXJycjBx4kQk\nJiZi7NixMJlMOH36NHbu3ImWlpbLvrdgTVp34tZbb8Wdd96JJ598EomJiXj00Uexbds2WCwW5sz5\nfD4MGTIE6enp0Ol0WLt2LUwmU5v3wvM8+vTpg3nz5mH69OnYsmUL7rjjDqSkpOC9995DXl5el+73\nsh2ey9n1BAuZCAaDAXa7vc2YXm9A6OQItSwulytAdBW8C6B4pVCkRVQ60X30GalUyhwM4TUu1cES\nOmZdYaSIxicGpqWlBX/961/hdrvhcrng8XigVquRmJiIpKQk7N69Gzt37mTxVwBwuVzs59DQUPj9\nfvTv3x8GgwHNzc0XULV2ux2hoaHs/t1ud4dt6G7GTalUQiaTwWazQaPRwOl0IjQ0FFarlRkDYhGC\nQf2q0WjYIkz3Tw5dsL6E2k/OX0hICOx2O6N5eZ6HQqHo1jZ2F64Eq0GLhlCcLJPJMHbsWDz//PNI\nSkpCbW0tFixYgL59+yItLQ0//fQT1qxZgy+++ALR0dF48MEHodPp4HA42NwMDheoVCqkp6fjgQce\nQHR0NAoLC7F69Wo4HA7cfvvtWLVqFd544w3s3LnzovqBK8H8/NYYJ7KlGo0GHo+H2RaHw4E+ffqg\npaWFJUUEQ9hOmUzGbGmwaF0ma13KhAkVfr8fDoeDvae95A2g1Za1Ne/bAs1/0qmQTdy5cycAoKWl\nBePGjcOCBQtw4MCBgO9TKpWw2+1ITU3FSy+9hOjoaDQ3NyM+Ph5FRUXw+/1wuVzYsWMHxowZg+ef\nfx61tbU4cOAAszOXiu5ycoSV80lYvGjRIjQ1NSEvLw9WqxUajQYtLS2sz+mZ/8///A/69esHrVYL\nmUyG999/n218gfN9olAo8O6776K5uRmLFi1CQ0MDysvLsXDhQni93naZoU63oSsPQEhZdWZxokaR\nA0GfsVgsLO7e22ElpVIJuVwO4PykAVongF6vZ4IyoZdMoEJ69HfhTpB+Fv7e5XIFDH6VSnXZbFK3\nKNYFbfH7/bBarXC5XAG6hoEDByIyMhJlZWUA2t85h4WFobGxEcXFxdBqtW1qIGh3ExzX7S24XC7Y\nbDZ2X+TEchyHsLAwZozbAu3+HA4HHA4H7HY7vF4vc2A7GrdarRYAWAYDobszMIBLdxLlcjlUKhVU\nKhXrG2HmSG+CxpZw7JDYPTY2FkeOHMF1110HuVyOCRMmwOl04ssvv0RZWRmqq6uRlpYGt9t9QbaL\n8JpSqRQ33XQT7rnnHoSHh+P48ePYuHEjtm/fjl27diEjIwNDhgzB5MmTO83q9Lbz8VtydoDzC6Td\nboff74dSqURSUhLmzp2L//zP/8RNN93Uri1QKpXQaDQBdli48FN4RC6XQ61WA2jta5vNBofDAbVa\nDbVa3SX2pj2Q7VcoFEhNTYXZbMaTTz6JP/zhD1i3bh1uuukmcBzHNjXkwAPAnDlzwHEctm7dio0b\nN8JqtWLEiBHYvHkz3n77bWzcuBHff/896uvrMXjw4As2ylcCZLdpLikUCvTv3x9nzpyB1WpFU1MT\n/H4/tFot2/T7fD7o9XoYjUZs3boVe/bswXXXXQej0XiBswMAcXFxkEqlyMjIYAwZkQ7dMe4vmeER\n6j44joNUKg1QVANgzgs1RtgwiUQCg8GAv/71rxg4cCCSk5Nx5MgR7N69G6tXrw4Ik/QG3G43W/y8\nXi8efPBBTJ48GSqVCvPnz2fvE8ZTiaajfxMTE1FbW4uQkBDW6cLOSU9Px7x58zBs2DA4HA5UVlZi\n6dKljNGi3U5nIHyWXZkA1Bf0s9B5EzJTN9xwAxISEpCfnw+9Xo+mpqY2F9URI0Zg165d+OSTT9ju\nmiYI0GocPB4PY0goLNabCAsLQ3NzM5YsWYL7778fLpcLOp0OjY2NmDVrFuLi4lBVVdXmZ4nNUalU\nCAkJgVarhdvtZu9XqVRt7jxCQkJgMpmwbds2VFdX49SpU3jzzTfZDrG7EcxmAO2nolLWRLCzDoDt\nXnsTVNKA5n9sbCweeughTJo0CXfccQcqKytx6NAh/PnPf8bo0aPxzDPP4PDhw/B4PNiwYQMee+wx\nTJkyBT/99BNrn/B5AK3PYvHixdDr9XjmmWdw6NAhZr+USiV27tyJpqYm/Nu//Rv+9re/dWo32ROh\ngmsJ9HyNRiMaGhowbNgwrFy5Ek6nE5MmTUJ+fj62bdvW5jh1uVyQyWSIjY1FSkoK4uLikJCQgKio\nKOh0Ouj1enAch4aGBixZsoQxD2TTnE4ns+3duZkWZlONGTMG8+bNw6uvvoozZ87A7/djw4YNeO+9\n93D27Fl8/fXX7DlIJBLExMTgpptuwjPPPIOjR48iISEBo0aNQlpaGj777DMArfMvOzsbGRkZmD17\nNrKzs5GXl9chS9XRvXYHPB4PSxTwer2YNWsWSktLsXnzZiiVShQVFUEqlSI8PBwWiwVKpRIcx2HJ\nkiXYsWMHVq9ejXvvvRe33nordDpdm/c1f/58vPfee4wt8/l80Gq1sNvtjDToCrqs4REOIlrkgPPe\nb7AR9fv9CA0NRf/+/XHw4EGUlZXhjjvuwNChQ7Fhwwa2A+8t0EMnj3TOnDkAgOTkZCxatAjr1q0D\ncL5CMA10juOQlpaGsWPHYvz48aiurkZYWBj27NmDgoIC5ObmIjo6GmFhYXjssceQnp6OxsZGSCQS\nTJgwAZ9//jn27t3LaPzOQvi8u2MCCxeFYDbm16wF+P1+1NfXM7q4LRiNRpSWlsJms3W4UGo0Grjd\nblit1l5fJMxmMziOwyOPPAKLxYJ58+Zh9OjRWLRoEebOnYuvv/46wEkTgp5TfHw85s6di+joaDQ0\nNGDdunUoKSlp16nw+XxQKpWIjIzEnj17oNfroVQqGZvUEztP4OLZRcD58UM7Zspwot8Jn0VvsAoS\nSWsdKKKtp02bhsmTJ2P79u04d+4cY50GDBiA5uZmnDx5kgk6jxw5gjNnzmDEiBH46aefoFQqLxiH\n1F69Xg+TyYSTJ08GhFfcbjcUCgXq6uqQmpoKrVbboYGl6/UmKx3sxLWXeRmMK8kKUYiSam4tW7YM\nBw8exLZt2/DBBx8gIiKiXQc7OTkZw4YNw4wZM5CYmIjo6GgoFAo4HA54PB40NzfD4/HgvvvuwxNP\nPAGr1QqVSgWFQgG73Q6ZTMZkA939DMgeDhs2DEajEY2NjZDJZHC73Th16hRkMhnGjBmDjRs3ss94\nPB7ExMSgubkZBQUF4DgOZrMZhYWFSE9PZ2Fur9fLNsf9+vVDUlIS8vLyLqsN3dluYfbjqFGjkJub\ni1OnTrG/CRlrr9eLmJgYDBo0CJs3b4bNZoPJZILD4YDVar1gMwIAKSkp+P7779n/yf44HI4O15/O\n4rIZHgLHcTAYDHA6nWwBI/pNWN8AaH0AMpkMw4cPh9lsxsqVK+H3+7F27Vo888wzWLx4MVatWtWr\nO0vqQJlMhjvvvBMlJSVYvnw5+vTpgw0bNiA1NRXLli1jbaWHnpSUhD179qCwsBArV65ESEgITp06\nhYceegijRo3Cc889h+HDh2PhwoV46aWX8MQTT6CxsRH9+vXDyJEjsWLFCkyePJntYDrbmcEhke4C\n7fKDw5OVlZVwu92oqKiAzWZr17inpqbi22+/hcvlYloXYT/SdWNiYuBwOGAymbrt3juLkSNH4v77\n78fLL7+Mbdu2wefzITs7G/v378dLL72EvXv3wmaztdkXHo8Hd911FxYuXIjjx49j69atmDBhAr78\n8ktkZWVh6dKlbTpwTqcTEydOxObNm/Haa68hIiICS5YsQUlJCXtel4u2vk+tVrNQWUfF+YDWPo+J\nicHMmTPRt29fTJ48GQ6HA/n5+VizZg2KiorYfO8N3Qixxl6vF6GhoXjyySdx4sQJLF++nOmjSkpK\nmEbMarUiLCwMZrMZBw8exGeffYZly5bho48+gtVqDbi2cGdsNBphtVqZcaaQi9frhUqlwvr167F4\n8WJERkYybWF7z7EzjmV3gUKOxBQI7dHVDCpOqlAo8Morr6C2thaPPvooKzyalJTEtFvB+Oabb5CU\nlISMjAx89NFHTCtiMpkCnrvZbMaCBQtgNBoxYcIEjBw5Ei0tLdi/fz8yMzOxdu3abnNMhVo9pVKJ\nWbNm4fvvv2eCeaCVrSwtLcWIESOg0WgYw2QwGDBs2DBkZ2cz/Z/dbse+ffvw5z//GRKJhGlH3W43\n9u3bB61Wi+uvvx5btmy5ouJ5kjlQaG7KlClYsmQJXC4Xm7NqtZq9x+PxICUlBQaDAQUFBWx+nThx\ngskJhPYFaGXKq6qq4HQ6WV2ikJAQmM3mLgm3CZfs8JBojBbH9PR0LFq0CD/88AP69+8PAHjllVeg\n0WjQ1NTEqH6iGt1uN6ZOnYr169ejsbGReXC1tbUwmUxt6oF60qDQ4A0LC8Of/vQnLFu2DDabDceO\nHcOGDRswe/ZshISEwGazMc/b5/PhgQcewEsvvYQ33ngDdrud1fb49NNPkZSUhI0bNyIpKQnLly/H\nP//5T7aTLykpQXV1NR5//HGkp6fj+PHjFxjn7kZ7z09IywIXqvkp3u7xeALuUSjI9vl8SE1NRUxM\nDA4dOsQMgcPhCDgolOd5GAwGpKSkoLi4GL/88gsbSx3d9+UaKfpun8/HQq2PPPIIvF4vduzYAafT\nCblcDofDAY1Gg927d+OBBx7AqlWr2mTc1Go1FixYAKlUihUrVsDj8SArKwtHjhzB7Nmzccstt+C7\n775j7xdO4Li4OGYMm5ubUVNTg0ceeQS7d+/u9IIVvLOnPqD5lZCQgD/96U+45ZZboFQq0dDQALPZ\njIyMDHz88ccsBAu0LpyJiYlIS0vDs88+i8bGRpw+fRqrVq3CyJEjMXv2bEyZMgVr1qzBhx9+CABd\nFgt2BkSXU58VFhYiJycHarUaTqcTKpUKZ86cQUxMDGsLMUIejwcnT56E0WhkjozQIRQ+O4fDgfj4\neDgcDiiVSvad9Ldvv/0WU6dOxS233IK3336bsV3BYeTutEuhoaFsF0tjV61Ww2q1wmg0wmQyBWic\n/H4/QkJCWHo+fYYWViFjR87clRQ7O51OfP755+jXrx9mzJjBKrgbDAacPHkSAALC3zR/XnjhBcTH\nx2PBggXIz8/HuXPnAJx/9vTe/Px8vPTSS9i1axdee+01mM1mxMbGIiMjAwqFAmvXru2waOWlgJhG\niUSCvn37QiaTBYRR6f62bNmCxYsXs0QFevXv3x9nz55lehiPx4OCggLI5XJER0ejurqaObVWqxUW\niwWpqakB/U993Nn77Sro2QGtzlxkZCSqqqpQXl4OuVzOqi7zPB+wmUhNTYXRaER9fT38fj8iIyMB\nADabLWAdkUhaa23Fxsay9HOe56HVapGYmIjS0tJucXi6HFNQKpVITk7GggULMHjwYNx444147LHH\nYLVaoVAoWDze5/OxhzBgwADU1tayxW7q1KkwGo3Izs7u9R0L0fkJCQmIjo5GWVkZYzuysrLQ0tKC\nkJAQ9n4qcT1mzBjk5uYycbPX62XsxtmzZ3H48GEAYGJfMphAK1tQWVmJ4cOHBwilrxbQIOQ4DhER\nEZBIJNDpdOxvtEBQzYS+ffsGHD8hfB9dRyKRIDQ0FImJiaipqUFtbW2vtYGEdP369cP+/fvhcDhY\nxgf9W1VVhcGDByM6OrrdaxoMBlRXV8PlcrEQ5y+//ILMzEyMHTs2gDYXZmhFRUXB4XAww1NeXg6Z\nTIbw8PAut5FCMjNmzMDdd9+NsLAwKJVK9OvXDykpKbj99tsDipjRwjh37lzMnz8fRqMR77//Pt54\n4w1s374d5eXl0Ol0SE1NxcyZM9l39YZgUiiIjIiIgEqlwtGjR5neied51NXVQavVorq6GsD5TEG/\n34+mpqaAsHp7qKurY3O6raw6s9kMu92OmJgYaDSaTt17VxYVjuOgVCoZA+73+5mdUSgUAYZeuNAJ\nxxTZLJqTAFjtIhLNX8mwllqtxvjx45lNpXtUq9VsM0B9QRoRr9eLH374ARs3boRGo8HUqVOh0WgC\nbKZSqWQb1r179+Krr77Cjz/+iLy8POzcufOimaCXA6Hjq9Fo4Pf70dzcHNAGABdsZEkrSk6q0F6Q\nlKNv377sfiljlOQSANpMsOktCOdJSkoKC+WTFpDEysI1PCQkBGFhYayGUHR0NKqqqti6K3Rcad7S\n2kKaWY1GA4vFcknSj/Zw2Q6Px+OB2+1GaGgoUlJSsH//fjz66KN46KGH8NBDD6FPnz4spMHzPNRq\nNRQKBSQSCaKjo1FRUQGPx4MnnngC/fv3Z9kWvQ0auKNGjYLD4WBhAJ1Oh6ysLOzduxdDhgwBgAAx\nNaUTyuVyNDU1sQ5SqVQAgFdffRUKhYLtZihDgRbg3NxcjBs3DmFhYT0iXu0qyBkYOHAgLBYLxo8f\nf4HhkEqliIiIQFpaGo4fPw63282YLqH2gwz44MGDMXbsWGRlZaGioqLH9CsEmjB+vx+TJk3Ct99+\ni82bNweUD/D5fHA6naisrERSUhIGDx7c5rUcDgfTdFA2iFKphMViQVZWFqZOnXrBdwNgz9BsNgew\nErW1tYiNje1yJpTL5cLw4cNx//33o6KiAg0NDXA6nYy5Sk5Oxrhx45hxoXGanp6OYcOGged5bN++\nHWVlZfB6vcjLy8N3332HxsZGjBs3jrFjvUGl04LAcRxGjRoFAMjJyWFG1O124+TJk4iOjmYbEb/f\nz4SpLS0tsFgsAWOvrVdGRgZsNhurA0XsDrXTbrejpKQEAwcORHR09EW1Zp3NUu0Izc3NSE5OZuxb\nZmYmDh48iNLSUuTn5+PDDz9E3759WViAHCSHwwGFQgGFQsHCmAkJCXjrrbfw5ptvQqlU9qpeLjhz\nl17/+te/8OSTT2Lp0qXsvocPH47m5masXLkScrk8IEuQYDKZUFFRgbvvvht/+9vf8Mknn0Cv10Mm\nk0GlUjGbnZycjP/6r//C5s2boVKpmJjZZDIhOzsber2+29pIawQt7PX19aipqQmYyxKJBM3NzVAo\nFGzOkaNjMpmQmpoKp9PJ+k4ikeDIkSNISEhgzoxKpQLP89i/fz/69evHsnolEskVcXgUCgWkUilC\nQkIwc+ZMZGVlAQBb5w0GA2svjTev1wuz2Qy/v7WSf1xcHKqrq1kEQJgg4/P5UFtbG2BrYmJiYDQa\nUVxc3C026LJmgdAwxcbGYv/+/di8eTM8Hg9KS0uxadMm3HzzzQBaPVKlUgmbzcZEmpSxM2TIEFx3\n3XV47rnn8MUXX0CtVvd6jJI6xmAwQCaToba2lnnsUqkU+fn5SE5OZgs50Lor4TgOEydORGRkJBN3\n0kKo0+lQXFyMoqIiJCcnswWWFl+O47Bu3TqMGDEC48aNu6I7r44QFhaGiIgIZGdnY9CgQazfgfN1\ndCiF9+eff2aTWjgZ6WeNRoNbbrkFUVFRyMzM7PE0S+EixvM85syZg3Xr1rFdgrAGEVHJDoej3Swt\nAgkmhYLeuro6yGQyaDQa5sQRTTtw4ED06dMHxcXFzNl1uVywWCxQqVRdGu+0+N1www0wGAxYsWIF\nbrzxRlx//fVYsmQJXn31VWi1Wvz3f/83YmJi2GcA4LnnnsMzzzwDs9nMWBG5XI4TJ07gl19+YfWJ\nqHZRbzE8dC8DBgxAWVkZfD4f3G43Y9QOHToEnU6HESNGwGazBYxHm80Gu93O6qK05ez4/X78+OOP\nqK+vx5AhQwIyE8lR93g8yMzMhFKpRHx8fKc2JF1xePx+P4YOHYqRI0fC7XZjxYoV+OCDD5Ceno7Y\n2FgMGTIEERERWLNmDcaPHw8AAeFFEsryPI/ExESsWbMGhYWFCA0Nxe7duzF06NBuT3ZoD205O1FR\nURg3bhw2bdqEkJAQhIaGIjU1Ff/85z/x0EMPYc+ePQFiV5qXwtfx48fx5ptvIj09nYXoaCzffffd\nyMrKQmlpKTt6pKWlBQMGDAAA5OXldWsyDM1xv9+PlpYWplcM1lfm5+ejvLwcWq0WUqmUlTlRKpUY\nPXo023SRA7N9+3ZERkYyx4JAVe9HjhzJwr29mclMIJJDJpMhJCQEp0+fZswMhasaGhpY2RmgNZnF\naDSyCE/fvn1RUVERkM0sZJ4jIyOZY8dxHIYPH47Y2Fi2qekqLtvtJ0rK6/XixIkTOH36NPPaTpw4\nAaPRyAwFsR5k7N1uN3Q6HUaNGoWysjLU1tZCqVT2et0PuieO41BfXw+73Q6dTsc8a6JPaUdBin+d\nToe+ffvi+eefxz/+8Q+kpaWxYmdOpxPNzc2sXovBYGDGlya1RCKByWSCVCpFdHT0VRnWAlr7jeM4\n1NXVsb4Ulhvw+/0ICwtjJ+UK61II30+UfVxcHORyOfP4e9PRi4qKgslkYnVntFptwGRTqVRwu93t\nhtpoUhuNRubwUn/KZDJUV1ezWDZBIpHAaDRCoVAEsIBUsNJisXSp70m/ExcXB7Vajd27d8NsNsNk\nMiEzMxNfffUV201SSBJoXVAKCgpw5swZxnLQgmCz2VBbW8ucN2LJegNk9GQyGbRabUAVbxp3JFaN\nioqCVCplDJTQIbtYPS+Hw8HCIG3ZHNJOyGQyViizI3TVgSBBb2hoKKKjo5GYmIjt27fD5XKxPli5\nciWTDNDxGgRqQ2hoKJKTk8HzPD744ANs374dKSkpWLRoEWMRhHOzt5CYmIjs7Gz4/X7YbDZYrVbM\nnz8fQ4cORV5eXoBGKlhjEx6NilT2AAAdnklEQVQezjYnZ8+eDQiHUQbkqFGjmO6F4Pf7kZSUhNLS\nUhw7doyF9boDwv4mlgYIDPkAgUUQ6dgSo9EIpVKJgQMHol+/fmzDExISgoMHD6K8vJw58vS3xsZG\nmM1mdhxPe5mkPQ2yl1qtFhqNBmq1mq2L1AYKvxJjZzQaERcXF+DstVV7jBynqKgoaLVaZgeIJPB6\nvVfG4RGGKziOw5QpU1BZWRnwHqqe6fF44PV6mbEHWh+EwWDAtGnTMG/ePHz88cds8SHdDy0K9G9P\ngwwnxczJcbNarZDL5YiJiYHNZkN4eDg++ugj/PTTT/B6vVi5ciUyMzPx/fffo2/fvlCr1YyWpUWU\nNDDUFhJmymQylJWV4frrrw8QHXYHPd7VZ0H3MH36dJSWlqKmpganT58GcF4XQLuSiRMnQq/XIzc3\nl6n3ic3jOI4ZBI1Gg0GDBiEnJ4cd/qjRaC67eujFQBoij8cDlUrFFi6Xy8WcUp7nWfHAyMhIOByO\ndh0enufhdrvh8XiYuJSqZ1P2RUpKCmOQ6DiO++67D16vl2V/kS5q3LhxrEheZxC8e+R5nh3dMW7c\nOOZg0xgEgKKiIpSUlIDjOMybNy8gRk5n8wgzkGi3WVZWxlgrCpX0Fuj5arVaWCwWds80Jyh9nOYR\nPW9yihobG5mRpM9RH9HYDg0NZQsKGXFikGiuFhQUoKamBpMmTWKarY7E/11x3p1OJ8rLy3Hffffh\n2WefRW5uLux2O9MEut1uZGZm4ty5c7jvvvvA83wAw0PPJiEhAcuWLcPf//53AMCWLVvwr3/9C9On\nT0d8fHzAGO4pUNiU2MzrrrsOb731Ft555x0oFApWCPDuu+/Gpk2bmNNPdiD4OdPG0O/3o7i4GCaT\nCeHh4WzzOW3aNNx6661YtWoVYyKVSiV0Oh0mTZrEqmXTmX/dYWvIEadaMgaDAXFxcawfyH6azWYc\nPnwYu3btQlZWFlatWoWnn34aY8aMwbFjxzBkyBAYDAZ4PB5YLBacPXsW3377Lfsej8cDnU6HM2fO\n4NVXX0VOTg40Gg1sNtslyQGCHbHLgd/vZzo6q9WKnJwczJ49m9lQoDVTbsaMGTh+/DgjQaZPnw6r\n1YpBgwZBqVSycDvdf3R0NIYPH46//e1vWLlyJfr06QO1Wg2/3w+3243i4mI0NDTAaDSytacruGRv\nQigEBYD4+HjU1tYyD00ikSA8PBy5ubkXPDChkPXGG2/EsGHDMGfOHHz55ZfMM6TOoYfYW7FnCrHQ\njooYjJqaGkydOhWJiYm47bbbMHXqVJw6dQqFhYUoLi7G119/jcWLF2PGjBkoLS1lxoT0IXq9nmVb\nUOYETWyr1Yr4+Pgr6uAEg8JvPM8jLS0NlZWV0Ov1OHLkSICH7fP5oFarkZKSAqvVCrPZjLCwMNTX\n119wTa/Xy3YDJO4TMiQ9AaFehYSFBBq/wudOlYbbW7h0Oh0rcAacp7Upfk3n49D1KXtt8ODB+PTT\nT9l76LmFh4d3WVvB861psUSZU0qrMM26pKQEY8aMYbF1mn90wrhQkE/jnqptcxwXIIztaQiPlgDA\n9BnAeQ0RbRyEFcGJIaAsUOFxHSSoFF5Dr9ez91A2jBBSqRRWqxVer5dllfQ0mpqamLH/5JNPAsYS\niVd/+uknLFmyhIVhhYJtABg7dizi4+Nx6tQpqNVquN1u7Nq1i9ULO3PmDHsmPcWuUn9Q30yaNAnx\n8fGoqqpiobfBgwfDbrdjx44dLAW7I1D/knCX+kan02HixIkwGAyora1lTrDb7UZcXBxGjBiBl19+\nudvLnBBT5na74XQ6sX//fmi1WtYPZEOjoqKg1+tRWVmJM2fOYMeOHaitrYXX62VaO4vFwkKwQoeM\nJAMUFaGSGVTF/kqtGRKJBBaLBV9++SUbR7QpOnfuHL755htYLBa24a+oqAAA5Ofnw2azweVy4S9/\n+QssFgv0ej3i4uLQr18/xMTEoK6uDl9++WVAqZa6ujqoVCokJyfjwIEDXU5quiz6RCgKlEqlqK+v\nZ8W7/H4/4uPj8eabb7L3kFaBDPGJEyeQlJSErKwsTJkyBf/7v/+LzMxM3HPPPYwZoDhlb2Rteb1e\nnDlzBhUVFYyus9vtkEgkqKurw9ChQ/H1118jOjoa8+bNw+nTp/HEE0+wsN3HH3+M2267DYWFhdix\nYwdb1Orr69GvXz8A5yk72lECQHV1NYYNG3bVODw0yGjCpqen48yZMxg4cCDeeecd5qQItSeJiYk4\nfPgwy8CiAxeB8wsMabeqqqpYdg0tpj0JMiI6nY45KETB0jEP5BTV1tZCrVYjISEBZ8+eDdD/AK3i\nOZ/Ph9WrV0MikbC0dpoLxOaRMzVkyBD8/e9/x+uvv46PP/4YKpWKpQbT8QjV1dVdYjBpxyWTyVBe\nXs6YRXIGAKCwsJAtGEK9kMPhgF6vh0ajYdWNqd+pzAIxdL0VcqXsORI60tk7VApCLpczZo00dcIx\nFnzGEjE7tKDQjvnhhx9GVFQU6/O29BCULWQ0GgNScttDVx0Iu92O06dPY8yYMcjJyWGLiHBXu2bN\nGkyZMgWpqakoKipizlBLSwu0Wi1mzZqFhoYGmEwmtoE8dOgQXnzxRSxfvhwZGRkBTmRPQJiYIZPJ\ncNddd6GhoYGx5R6PB0899RS2b9+OH3/8kfVLe7XISEtGpUAUCgVjYW02GxYsWICamhqcO3eOOb1S\nqRSLFi1Ceno68vPzmYPQXU4eXcftdkOtVuPnn39G//79cfLkyQAhbnV1Nd544w1WHNFgMDBngPqA\nngk5axRypKwmcmrJVlHxxt4iAtpqt1KpZNolnueZvaioqMCdd94Z0J9kW+hw1hUrVuDpp59GQkIC\nnE4nfvjhB+Tl5SE7OzvAySdb5HA4sGfPHvB8a/2si2ksL4ZLfmrBD1qtVrMiSTTgRowYgfr6emaU\nnE4nY1BcLhfWrFmDo0ePYsmSJfjDH/6AP/7xj/D7/aiursbDDz/MOl+4e+kpkNEoKSkJUM8DrR3s\ncDjQt29f7Nq1C9OnT8e+fftgNptx9OhRDBw4EEqlEvv27YNOp2OTHGh1os6dO4eWlhb2O6qTQUa5\nubkZdrudiZ2vNIQGp3///tDpdNBqtdi+fTtaWlrYokMeeHR0NMs6mDx5MioqKlgRKupv0u/QNYcO\nHYqYmJgen7AUEgXOVzelNgoXODKQdJCokJomFkihUOD//u//sH79epw8eZLpJ+gwO6DV+E2ePBmj\nR4/GunXrkJGRgUOHDmH16tUBWR1SqRS33norY4q6Mr5JoOlwOHDs2DH23OneKPVdKpVCrVYzvRI5\nBrfccgtKS0vZokpzjvQ8VNa9N0Hi/2+++QbTpk3DoEGDApwanudx9OhRcByH8PBwxg6QY2OxWJCQ\nkMCKvVEWVnR0NK6//np88sknGDhwIDZu3IiCgoI2nR3qf7fbjaioqAC9TPD7umOzQgaeToGnKsKk\nY6BNUk1NDXJzczF+/PiARYEObCRxPC2cpNsqKirCiBEjLtDh9QTInhJDHhsbi++++44xLxzHYdiw\nYfjiiy8CNh3tbWxbWlrAca1V7QcNGoRPPvmE9dmQIUOgVCqxbds2NhdUKhU0Gg3uvfdefPDBByy8\n3p1Ou9COu91u/Pzzzzh16tQF2akKhQINDQ1s/NJxOsI5L0zrpuuRU0N6F3IqiIHlOI45RL0J2rCR\nBITWBGKWg+0PcH7TS4ez7t27F7fddhvmzJmDu+66Cx999BGOHTvWZpFav9+PhoYGvP7661i9ejWq\nq6u7vFZe8qojZCiAVqqfvDmiWiMiIgCc18ZQI2jQnTlzBv369WOLS3Z2Nl555RUYDAYsXbqUTVTa\nwfYkaPDRWR16vT7Ag7bZbPD5fMjJyWFibBqkdLBdeXk5mpqaYDAYmNNHWgRqB32X8PlR2OtqS0sn\nTYparcaxY8ewZ88eFoaj+iA8z7MYNrEWVVVVF/QXDVybzYbGxkZ2yFxvsDsECiW2FfcmllKlUjHR\nsVBHoNVqodPpMHnyZBw9ejSAZRCOaY/HgwULFmDJkiW46aab0NDQgA0bNrAqs8JieHq9Hmazuctt\nFD5rKtUubAOlcgufAzlfMpkMCQkJF5z9RqyIkB0Jfp49BXLEHA4HcnNzWdG9YBQWFqKmpgZNTU2s\nDhEVBa2srITRaERoaCi75/DwcMydOxePP/44UlJSsGPHDmzdurVdp5sElxKJhIWkexK0MTh9+jTL\n6iGGKbgez8GDB5GYmAi32w2NRsM0abQ45ufnB7BDANihxaRj68lNJNkDcsQoy1MmkwVoG6neE91P\ne04YjelRo0bBarUiKyuLMZZJSUkwm83siBCgVThrNBrhdDqxe/dudp3uzP4VsojkfFRWVjL7Esww\nUgYk1XQShpGF9xXsQJNOixxusjVXQrAMgGlr7HY7K6YItM9uBssWKHFHWEKBSknQMxKGRMnRA86v\nI1119C5LwyP82efzsQqLLpcrIBuGsiHovdSZpaWliImJweOPP44XXngBDQ0NqK+vx/jx4zFhwgS8\n8847WLBgARssPZ2qTqK2zMxMTJs2Dfv27UNJSQkAsLhjfn4+1Go1TCYTOI5DUVERrFYrBg8ezGoK\n0REb1OaysjIWo6bFBgATu7a0tKCqquqyaeaeWIRkMhl0Oh3mzJmD0tJSfPjhh6irqwMQWG2X51sL\ntNXX1yMiIgKfffZZwAF9wiM7PB4PampqsHDhQpjNZubsXOxIjc6KQYPpanI46TyllpYWqNXqgFAP\nOa1UCKympgb9+/fHvffei5ycHHg8HowZMwaPPPIIxo4dizVr1iAnJwfAeZZTGFL593//dzz55JNo\naWnBwoULcfr0aRa+Ezp3KpUKtbW1KC4uZt9/uXC73QgPD4ff72ehRKH+hlKwH3zwQRgMBhb+oo3I\npEmT8PPPPwMAE3wSTU/MiMFgQGNjY6/NQwCssvlHH33EanORsJWOJ6C+JTtD/Uu0OSVSeDwexMbG\nwmw2o6SkBPfddx+OHDnC5ltbomxiEPr374+CggJYrdaAudbWmOzqxkwul6O+vh42mw0DBgzA4cOH\nA8SmFNooLy/Hgw8+yMIFtBAolUoYjUacO3cOVquVMR6kNTl06FC33WtHoH4yGo2IjY1Ffn4+srOz\nmf2Piopimw8SmAefbi9kiKZPn46bb74ZI0eOxOLFi1m/3nDDDaxaOdWDIab29ddfxwcffIC8vDxW\nq6mnNlhkD4OrkAsXaxpf9G9wivzF+oPG4+VqHoXh+a6sGcHP8GL3096a1lZf0OZf+H8humvMdonn\nowEmFAkaDAbmABDCw8PZwgO03nxTUxOGDx/OdstyuRx5eXkoLi5GZGQk/H4/M3y9herqakRHRzPD\nAbQyPySwUqvVsNvt4DgONTU1UCqViImJYVQlTVxagBsbG1noQggKK5SWlkKr1fZqFkxHoMWXan+U\nlZUx0RkQOIDlcjnq6urw+eefs4Pw6BpCA0a7N6fTyTK9gEAxaU+BnC2n08nodGEbaMdBaciNjY1Y\nvHgx0+PceOONqK6uhslkwltvvcW0XSqVio0DclrKysrwzjvvwOFwoKKi4oLvITidTvz4448slt8V\nJ4IyRIThKFooqC+bm5svCLVSWjc5XxQ2IaqePut0OtmutKeZVgIJkgGwAweFYmsSTQJgmZXA+bFJ\nDgrNS9J8bNq0CYcPH8axY8cC2tKeUSZm4fjx4+3ea3c+E6/XC6vVCqfTycKvwSBmgJgb6kug1ZFu\nampihRfJ8QDAxPa9ocUiW9bc3Iy4uDjY7XaWpdnS0sJ+HjFiBLKyshi7KNRJ+Xw+JCUl4brrrsN/\n/Md/ID4+HqtWrQooTJuWloYBAwbgww8/RGNjI4suOJ1OxMfHIz8/H+Hh4ayg5u8ZQsf5SpR+uZpw\nyTOARFZA60Qym80Bi3piYiLq6+sRGRmJPn36QK/Xw+/3w2KxoKqqig1OmUyGadOmsc5wuVzQarVo\namrCiy++yLQEwu/rKdBCUVRUhMTERIwfPx45OTlsZ5Wfn4/IyEgcPHgwYMDcfvvtGDt2LOrq6iCR\nSJhehxiGTz/9FF999RVLyyfjSu/Ztm0bfvzxxytGUQaDDGRTUxO2bduG4uJiAIFiZgKJRClsQ3R1\n8AJODgWFCGjhosWrpxZSylJRKBTw+XxYv359m8+ZnJiGhgbMnDkTs2bNQp8+fVBWVoannnoKBQUF\nLHNIqCUBwNgFAMyBJZBehpxb2qG7XC689tprUCqV3VJAjBi5sLCwgLAUtTU8PJyFXolplMvlmDx5\nMpRKJY4ePcoWBCEFT2G+QYMG4ezZs73ilAfbFsr4I5aC9ID0Xlr4KbQDAOvXr8e6desYkyWRSC6g\nwYU7XqEjRX/z+/0wGAzYt28fNm3axFiIjtCVnTPNi8rKSpSXl2PMmDEBc4P+pRo0M2fOxMKFC7Fx\n40Ym4G5ubkZeXh4LWwnD5hzHobi4uF0tUneCnjvpabxeLyIiInDu3DnIZDKUlJRg69atWL58OXJz\nc/HZZ5/BbrfD6XQiPDwcer0eCxcuxPjx4+FwOPDyyy8jOzsbTU1NAM6LmGfNmgW/34+tW7cyxpzY\nyZCQEHz//fesam/wRuf3huCx/XvGJTs8wYaPxMa0cCgUCowaNQq7du2Cz+dDY2MjWlpaoNPpoNFo\nEBoaCp/Ph6amJsjlcrzxxht4+umnAQRSfSQW7ekDC4HAGh6nTp1CSkoKALDMsy1btiAyMpKdsux0\nOmGz2XDq1CkkJiZi8eLFePbZZ3H8+HG2KJID0NzcHHBuE525RcaYavBcDQORGIuKigp8+umnjK0i\nxoBYhGAWh0TmbbVBWPuFjKEw7b+nQBk/QOsYffbZZwP+TqyG0EkoLy/H2rVrA5wQjuPYOOA4DqGh\noWhqamLOC2lmSAwtkUhY4Uqn0xng4AmrAgvHwOWCCh7W19cHLMhCJ5Li6MJsGJVKhfvvv58d0Env\no76ur69HUVERYmJiEBcXxxaTnkZbh35SqrnQAaA20fMUOjSkiRNqWII3G/Ss2nr2pKOwWCx4+eWX\n2VjoSGRPju3lgtplMplQWVmJiRMnMhZROO/mz5+PRx55BIWFhfjHP/6BpKQkrFixgtkQAJg9eza+\n/vrrgMNCeZ5HZWVlAAvYU6ASHCqVCtnZ2di/fz+A8/osiUSCpUuX4o9//CNuuOEGrF69mh3gWldX\nx2oOvfDCC6iqqmLlLChcSdW/FQoFMjMzGTtElY5dLhfmz5+PmpoaJi/oLXbyakWw4/x7xmVxnGRQ\nyKh4vd6A6pdRUVHYsmULsrKyWCaERqNhNOTIkSMRHR0Ni8WCtLQ0VkiJxHlqtZotKr0BEml6vV5W\nFwE4ryEpKipiokHKOmppacGrr76KlJQU2O125OXlBbA/tMAQm0WTT5gBJDTgV4PDExyGERpHKshI\niyIZeeHiQU4ELfIAAhzA4F1ye8a3u/qdnA4KbdH5Q0R/084yNDQUHMehqamJOS08zzNHh07q9ng8\nzFHneT7A6SNxstvtZqwP0fTBpfCFmRxdAS1q5KwQiHEDWlkBqoMUGxuLPn36ICoqComJiayqa0hI\nCAvR0f2ZzWZWYbw3DSXNBWJ1aNxQHwrnCjk+pC2jfhOGegjkyLTnGApBGy2yW+0JJYU75+4Ysz6f\nj2VACjOqaL7RSdKRkZGoqanBzTffjLfeeottqnw+HzQaDXtmNFcp27Q3Qjt0YCbNDZ1Ox7J5aD5Y\nrVZkZGRg7969mDBhAkJCQtDU1ITi4mKUlJSgsrKSjUvaBNNcpOriv/zyCw4ePMhSumlskL0WF3cR\nbYIW9bZeAPi2XhqNhgfASyQSfuHChXxiYiL7m0Qi4dPS0ni5XM4D4BUKBQ+A5ziOl0gkvFQq5QHw\nISEhfHh4OB8bG8s+y3Ecz3Fcm995Oa+O2iZso0QiYd87dOhQ/qmnnuIHDhzIS6VSnuM4XiqV8lKp\nlJfJZBe8QkJCeKVSeUEbhK/uak9X2vhbfFG/XKvt60of0rjkOI5/7rnn+KFDh7b57GQyGb9p0ya+\nsLCQP3r0KH/gwAH+l19+4d99911++vTp7FoSiYQHwKvVah4Av2rVKj43N5f/+OOPeZlMFjDGr7Vx\n2tacvZQ53Nlx2l4bySYC4O+44w7+hx9+YP9XKBS8VCrlR44cyb/44ov8zJkz+R07dvDz58/n9+zZ\nw8+cOZNXKBS8UqnkMzMz+XXr1l1gjwDw6enpfJ8+fXgAvEwm67F+bO/5BL86+762XlKplFepVKxt\nNH4vdv2LvSQSyVU9Ti/3FfxsurONne3b7rr3zl6/vXZdFsNDuxmNRoP169ez31OMODc3l+06hVQ4\n7f5J+0BnhpCHHlz9tjcgpIxVKhXy8/ORn58fsNsNDQ1FS0tLwE6O/iVRpTBU0xvo7eck4uoDsWev\nvPLKBaJsGoc8z+PZZ5/FgAEDoNfrceLECTidTlRVVcHj8UCv16OpqSkg5KBQKPDiiy9i0KBB7GgM\nSnu+GpjI7saVZgOITZVIJCguLoZGo2EV2d1uN1QqFd5//31s2rQJEomEVbRVq9V48cUX8e2336Kw\nsBAKhQJbtmwJqFRLbTt27BjsdnuvHQQb3L7ufh8xsTRur3Qf/hZwKWtGZ9/blefe1ncEX6+7+/WS\nHR6h0JKqERPdTBO0LRqYCtYBgTogYToaTfreHLzBBQ6FDhD9nkJZPM+3GY4QGpDedEREp+f3DZor\nwnHQ1vyhCtc+n4+dOqxQKFhpBPqcsKhkeXk5K38PnK8q25tZk78XkKPq9/tRVVXFnE5hWnNVVRVu\nuOEGpKSkIDc3Fz6fD4cOHYLH48Ftt90GuVyOHTt2sDIDwZoi2ngKhfa/VQhtcE+XSrjW8HtfMyQd\nORe/0kcB6EhvQn8TltwXOgnBEMange4tDvXrd1+0d3+lyej9F9yfkLVp61mRI0QLT28yPL9qaDrV\nxt64p+4GOZ0+n6/DNv5W20fo7DgV/p90SMGsi3AsC5120ngEZ9LR0QrB41ZYcJDmaFeEy5fTxt8K\nOjtOf31vh20MDQ3Fu+++i6VLl7LsTrIrWq0WCoUClZWVrFAm6SijoqLQ2NgInU4Hk8nEGB76lzRR\n9O/l4mL92Bt9SLaPxuyvz75bruv3+6+5cSqMSvx65t9F2/hrmPWy0NYa2N7aebH3dYYFagvt9eMl\npxYImRDhDZOTQ0JCYm5IgEhVFIHzGSEkbqX3Uoplb4IEqPQQg2tVtMXo0ISjv3fkEPUUfu+euggw\nNgYIPOWeEOysC50gcmioOCjV0iKxs8fjYcJXoeBaRM/CarVi+/btaG5uZlmctGmsr69nhfecTic7\nfJbnedTU1EAikcBkMgFof+G4Wmp+dQW0ZpDtvRbDrD2BnlgzyJ4IX1fi+p29h0tmeLoL7bEqwTS9\nMNzU0Wfa+v3VvqsM3o3Tz8K/t/d8CFd7Gy8HwQP2YrsuEozSblbIHArrcNAOR5g5JqzDEjz26BrC\nYpJt3V979w107AQLnJFrrg+DcbW3MXj+Cf8vZLU7YnEvt41CW0dZnXQcxNWGi7WRmIHe1DO2hUud\ni5fC0v0qlr7g2sJx0pa2qLuzcTsai8LvojMtZTIZZaNetI3Lli3jNRoN9Ho95HI5ZDIZIy7oIFCd\nTscyW0NCQlh2Ien9SMNLpT3Igec4jlXbJltMDqzQBgcTJECrpIbY5oaGBqa9JdtP3zdlypQ229g7\nxyAHoSMvrLtFUFczgo1s8ORo61m059xdS7iYY9FZBAvMAQSEUGmCCcOSQODhisLaORd79sH9SEYw\n+PfBLIyInsHFxs+lPPu2NifdBaGBp/Ozfmu27Eoj+Hl1ZFs7+tzlfBddX7iu0QZM6GgI627Rws7z\nreUvhKUkeJ5ndYWE3ydktej7qL4ZAHbcCB01QqL18PBwSCStZz929tzGt99+GyqVipUeoZIAwnpj\nVNTV5XKx8+bIoSHbR/dMkRsqC0JH+gQzx3TNtsTLPH/+3EOqZ0fPR7h5FRYlvaDvRIMrQoQIESJE\niLjWcfnlQUWIECFChAgRIn4jEB0eESJEiBAhQsQ1D9HhESFChAgRIkRc8xAdHhEiRIgQIULENQ/R\n4REhQoQIESJEXPMQHR4RIkSIECFCxDWP/wdwZ0kdbYDkDAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f420280cf60>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "xOt_LvYv6O_b"
      },
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Network"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "uaT6lQEK6hVw"
      },
      "cell_type": "markdown",
      "source": [
        "## Build"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tobvxLB-zseX",
        "outputId": "51ceefce-07ac-49eb-80f7-c3e363fb8572",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "cell_type": "code",
      "source": [
        "# Settings\n",
        "pool_size = (3, 3)                  # size of pooling area for max pooling\n",
        "prob_drop_conv = 0.25              # drop probability for dropout @ conv layer\n",
        "prob_drop_hidden = 0.3              # drop probability for dropout @ fc layer\n",
        "\n",
        "# Convolutional model\n",
        "model = Sequential()\n",
        "\n",
        "# conv1 layer\n",
        "model.add(Conv2D(8, (3, 3), padding='same', activation='relu', input_shape=input_shape))\n",
        "model.add(MaxPooling2D(pool_size=pool_size, strides=(2,2), padding='same'))\n",
        "\n",
        "# conv2 layer\n",
        "model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=pool_size, strides=(2,2), padding='same'))\n",
        "\n",
        "# conv2 layer\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=pool_size, strides=(2,2), padding='same'))\n",
        "\n",
        "# fc input layer\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(prob_drop_hidden))\n",
        "\n",
        "# fc1 layer\n",
        "model.add(Dense(5120, activation='relu'))\n",
        "model.add(Dropout(prob_drop_hidden))\n",
        "\n",
        "# fc2 layer\n",
        "model.add(Dense(nb_classes, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 60, 78, 8)         80        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 30, 39, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 30, 39, 32)        2336      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 15, 20, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 15, 20, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 8, 10, 64)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 5120)              0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 5120)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 5120)              26219520  \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 5120)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 20)                102420    \n",
            "=================================================================\n",
            "Total params: 26,342,852\n",
            "Trainable params: 26,342,852\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "pjdhi6sg6jpH"
      },
      "cell_type": "markdown",
      "source": [
        "## Train"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "OQCcrF3kzsec",
        "outputId": "03b7c72a-fd04-4923-d97d-4dcd888e9a50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6837
        }
      },
      "cell_type": "code",
      "source": [
        "# Settings\n",
        "batch_size = 128\n",
        "epochs = 200\n",
        "\n",
        "# Train\n",
        "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, shuffle=True, verbose=1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "2000/2000 [==============================] - 2s 799us/step - loss: 3.0000 - acc: 0.0555\n",
            "Epoch 2/200\n",
            "2000/2000 [==============================] - 1s 342us/step - loss: 2.9885 - acc: 0.0625\n",
            "Epoch 3/200\n",
            "2000/2000 [==============================] - 1s 354us/step - loss: 2.9799 - acc: 0.0635\n",
            "Epoch 4/200\n",
            "2000/2000 [==============================] - 1s 356us/step - loss: 2.9745 - acc: 0.0775\n",
            "Epoch 5/200\n",
            "2000/2000 [==============================] - 1s 360us/step - loss: 2.9654 - acc: 0.0865\n",
            "Epoch 6/200\n",
            "2000/2000 [==============================] - 1s 354us/step - loss: 2.9550 - acc: 0.0965\n",
            "Epoch 7/200\n",
            "2000/2000 [==============================] - 1s 359us/step - loss: 2.9448 - acc: 0.1050\n",
            "Epoch 8/200\n",
            "2000/2000 [==============================] - 1s 351us/step - loss: 2.9308 - acc: 0.1025\n",
            "Epoch 9/200\n",
            "2000/2000 [==============================] - 1s 354us/step - loss: 2.9180 - acc: 0.1135\n",
            "Epoch 10/200\n",
            "2000/2000 [==============================] - 1s 366us/step - loss: 2.8987 - acc: 0.1195\n",
            "Epoch 11/200\n",
            "2000/2000 [==============================] - 1s 354us/step - loss: 2.8632 - acc: 0.1555\n",
            "Epoch 12/200\n",
            "2000/2000 [==============================] - 1s 357us/step - loss: 2.8325 - acc: 0.1575\n",
            "Epoch 13/200\n",
            "2000/2000 [==============================] - 1s 367us/step - loss: 2.7930 - acc: 0.1740\n",
            "Epoch 14/200\n",
            "2000/2000 [==============================] - 1s 355us/step - loss: 2.7423 - acc: 0.1805\n",
            "Epoch 15/200\n",
            "2000/2000 [==============================] - 1s 358us/step - loss: 2.6803 - acc: 0.1830\n",
            "Epoch 16/200\n",
            "2000/2000 [==============================] - 1s 357us/step - loss: 2.6173 - acc: 0.2070\n",
            "Epoch 17/200\n",
            "2000/2000 [==============================] - 1s 353us/step - loss: 2.5806 - acc: 0.2070\n",
            "Epoch 18/200\n",
            "2000/2000 [==============================] - 1s 357us/step - loss: 2.5337 - acc: 0.2210\n",
            "Epoch 19/200\n",
            "2000/2000 [==============================] - 1s 360us/step - loss: 2.4936 - acc: 0.2390\n",
            "Epoch 20/200\n",
            "2000/2000 [==============================] - 1s 371us/step - loss: 2.4701 - acc: 0.2380\n",
            "Epoch 21/200\n",
            "2000/2000 [==============================] - 1s 374us/step - loss: 2.4212 - acc: 0.2585\n",
            "Epoch 22/200\n",
            "2000/2000 [==============================] - 1s 352us/step - loss: 2.3984 - acc: 0.2465\n",
            "Epoch 23/200\n",
            "2000/2000 [==============================] - 1s 375us/step - loss: 2.3640 - acc: 0.2645\n",
            "Epoch 24/200\n",
            "2000/2000 [==============================] - 1s 390us/step - loss: 2.3413 - acc: 0.2760\n",
            "Epoch 25/200\n",
            "2000/2000 [==============================] - 1s 364us/step - loss: 2.3088 - acc: 0.2990\n",
            "Epoch 26/200\n",
            "2000/2000 [==============================] - 1s 394us/step - loss: 2.2882 - acc: 0.2895\n",
            "Epoch 27/200\n",
            "2000/2000 [==============================] - 1s 354us/step - loss: 2.2510 - acc: 0.3080\n",
            "Epoch 28/200\n",
            "2000/2000 [==============================] - 1s 367us/step - loss: 2.2327 - acc: 0.3085\n",
            "Epoch 29/200\n",
            "2000/2000 [==============================] - 1s 398us/step - loss: 2.1997 - acc: 0.3055\n",
            "Epoch 30/200\n",
            "2000/2000 [==============================] - 1s 393us/step - loss: 2.1739 - acc: 0.3365\n",
            "Epoch 31/200\n",
            "2000/2000 [==============================] - 1s 377us/step - loss: 2.1452 - acc: 0.3295\n",
            "Epoch 32/200\n",
            "2000/2000 [==============================] - 1s 378us/step - loss: 2.1460 - acc: 0.3370\n",
            "Epoch 33/200\n",
            "2000/2000 [==============================] - 1s 382us/step - loss: 2.1172 - acc: 0.3340\n",
            "Epoch 34/200\n",
            "2000/2000 [==============================] - 1s 370us/step - loss: 2.0640 - acc: 0.3630\n",
            "Epoch 35/200\n",
            "2000/2000 [==============================] - 1s 363us/step - loss: 2.0581 - acc: 0.3705\n",
            "Epoch 36/200\n",
            "2000/2000 [==============================] - 1s 356us/step - loss: 2.0287 - acc: 0.3785\n",
            "Epoch 37/200\n",
            "2000/2000 [==============================] - 1s 374us/step - loss: 2.0005 - acc: 0.3865\n",
            "Epoch 38/200\n",
            "2000/2000 [==============================] - 1s 379us/step - loss: 1.9726 - acc: 0.3985\n",
            "Epoch 39/200\n",
            "2000/2000 [==============================] - 1s 381us/step - loss: 1.9335 - acc: 0.4075\n",
            "Epoch 40/200\n",
            "2000/2000 [==============================] - 1s 370us/step - loss: 1.9075 - acc: 0.4150\n",
            "Epoch 41/200\n",
            "2000/2000 [==============================] - 1s 388us/step - loss: 1.9207 - acc: 0.4095\n",
            "Epoch 42/200\n",
            "2000/2000 [==============================] - 1s 385us/step - loss: 1.8915 - acc: 0.4130\n",
            "Epoch 43/200\n",
            "2000/2000 [==============================] - 1s 374us/step - loss: 1.8435 - acc: 0.4425\n",
            "Epoch 44/200\n",
            "2000/2000 [==============================] - 1s 355us/step - loss: 1.8148 - acc: 0.4415\n",
            "Epoch 45/200\n",
            "2000/2000 [==============================] - 1s 363us/step - loss: 1.8283 - acc: 0.4250\n",
            "Epoch 46/200\n",
            "2000/2000 [==============================] - 1s 387us/step - loss: 1.7786 - acc: 0.4550\n",
            "Epoch 47/200\n",
            "2000/2000 [==============================] - 1s 382us/step - loss: 1.7347 - acc: 0.4705\n",
            "Epoch 48/200\n",
            "2000/2000 [==============================] - 1s 377us/step - loss: 1.7309 - acc: 0.4690\n",
            "Epoch 49/200\n",
            "2000/2000 [==============================] - 1s 366us/step - loss: 1.7228 - acc: 0.4775\n",
            "Epoch 50/200\n",
            "2000/2000 [==============================] - 1s 376us/step - loss: 1.6851 - acc: 0.4815\n",
            "Epoch 51/200\n",
            "2000/2000 [==============================] - 1s 398us/step - loss: 1.6556 - acc: 0.4830\n",
            "Epoch 52/200\n",
            "2000/2000 [==============================] - 1s 364us/step - loss: 1.5937 - acc: 0.5000\n",
            "Epoch 53/200\n",
            "2000/2000 [==============================] - 1s 356us/step - loss: 1.5657 - acc: 0.5115\n",
            "Epoch 54/200\n",
            "2000/2000 [==============================] - 1s 352us/step - loss: 1.5463 - acc: 0.5200\n",
            "Epoch 55/200\n",
            "2000/2000 [==============================] - 1s 375us/step - loss: 1.5011 - acc: 0.5360\n",
            "Epoch 56/200\n",
            "2000/2000 [==============================] - 1s 364us/step - loss: 1.4787 - acc: 0.5430\n",
            "Epoch 57/200\n",
            "2000/2000 [==============================] - 1s 355us/step - loss: 1.4318 - acc: 0.5525\n",
            "Epoch 58/200\n",
            "2000/2000 [==============================] - 1s 353us/step - loss: 1.4055 - acc: 0.5645\n",
            "Epoch 59/200\n",
            "2000/2000 [==============================] - 1s 359us/step - loss: 1.3776 - acc: 0.5700\n",
            "Epoch 60/200\n",
            "2000/2000 [==============================] - 1s 361us/step - loss: 1.3475 - acc: 0.5820\n",
            "Epoch 61/200\n",
            "2000/2000 [==============================] - 1s 366us/step - loss: 1.3070 - acc: 0.6120\n",
            "Epoch 62/200\n",
            "2000/2000 [==============================] - 1s 357us/step - loss: 1.3111 - acc: 0.5950\n",
            "Epoch 63/200\n",
            "2000/2000 [==============================] - 1s 357us/step - loss: 1.2923 - acc: 0.5910\n",
            "Epoch 64/200\n",
            "2000/2000 [==============================] - 1s 381us/step - loss: 1.2475 - acc: 0.6110\n",
            "Epoch 65/200\n",
            "2000/2000 [==============================] - 1s 372us/step - loss: 1.2469 - acc: 0.6110\n",
            "Epoch 66/200\n",
            "2000/2000 [==============================] - 1s 357us/step - loss: 1.2041 - acc: 0.6170\n",
            "Epoch 67/200\n",
            "2000/2000 [==============================] - 1s 382us/step - loss: 1.1866 - acc: 0.6275\n",
            "Epoch 68/200\n",
            "2000/2000 [==============================] - 1s 356us/step - loss: 1.1453 - acc: 0.6415\n",
            "Epoch 69/200\n",
            "2000/2000 [==============================] - 1s 366us/step - loss: 1.1078 - acc: 0.6555\n",
            "Epoch 70/200\n",
            "2000/2000 [==============================] - 1s 377us/step - loss: 1.1111 - acc: 0.6470\n",
            "Epoch 71/200\n",
            "2000/2000 [==============================] - 1s 365us/step - loss: 1.1187 - acc: 0.6380\n",
            "Epoch 72/200\n",
            "2000/2000 [==============================] - 1s 363us/step - loss: 1.0414 - acc: 0.6800\n",
            "Epoch 73/200\n",
            "2000/2000 [==============================] - 1s 362us/step - loss: 1.0220 - acc: 0.6930\n",
            "Epoch 74/200\n",
            "2000/2000 [==============================] - 1s 355us/step - loss: 0.9897 - acc: 0.6905\n",
            "Epoch 75/200\n",
            "2000/2000 [==============================] - 1s 370us/step - loss: 0.9795 - acc: 0.6965\n",
            "Epoch 76/200\n",
            "2000/2000 [==============================] - 1s 371us/step - loss: 0.9493 - acc: 0.6990\n",
            "Epoch 77/200\n",
            "2000/2000 [==============================] - 1s 386us/step - loss: 0.9359 - acc: 0.6980\n",
            "Epoch 78/200\n",
            "2000/2000 [==============================] - 1s 361us/step - loss: 0.9115 - acc: 0.7125\n",
            "Epoch 79/200\n",
            "2000/2000 [==============================] - 1s 369us/step - loss: 0.8817 - acc: 0.7330\n",
            "Epoch 80/200\n",
            "2000/2000 [==============================] - 1s 376us/step - loss: 0.8867 - acc: 0.7200\n",
            "Epoch 81/200\n",
            "2000/2000 [==============================] - 1s 352us/step - loss: 0.8664 - acc: 0.7255\n",
            "Epoch 82/200\n",
            "2000/2000 [==============================] - 1s 367us/step - loss: 0.8300 - acc: 0.7380\n",
            "Epoch 83/200\n",
            "2000/2000 [==============================] - 1s 370us/step - loss: 0.8297 - acc: 0.7335\n",
            "Epoch 84/200\n",
            "2000/2000 [==============================] - 1s 372us/step - loss: 0.8154 - acc: 0.7415\n",
            "Epoch 85/200\n",
            "2000/2000 [==============================] - 1s 366us/step - loss: 0.8256 - acc: 0.7315\n",
            "Epoch 86/200\n",
            "2000/2000 [==============================] - 1s 369us/step - loss: 0.7763 - acc: 0.7430\n",
            "Epoch 87/200\n",
            "2000/2000 [==============================] - 1s 382us/step - loss: 0.7165 - acc: 0.7765\n",
            "Epoch 88/200\n",
            "2000/2000 [==============================] - 1s 363us/step - loss: 0.7216 - acc: 0.7725\n",
            "Epoch 89/200\n",
            "2000/2000 [==============================] - 1s 355us/step - loss: 0.7003 - acc: 0.7725\n",
            "Epoch 90/200\n",
            "2000/2000 [==============================] - 1s 356us/step - loss: 0.7120 - acc: 0.7655\n",
            "Epoch 91/200\n",
            "2000/2000 [==============================] - 1s 375us/step - loss: 0.6705 - acc: 0.7905\n",
            "Epoch 92/200\n",
            "2000/2000 [==============================] - 1s 376us/step - loss: 0.6584 - acc: 0.7930\n",
            "Epoch 93/200\n",
            "2000/2000 [==============================] - 1s 376us/step - loss: 0.6535 - acc: 0.7950\n",
            "Epoch 94/200\n",
            "2000/2000 [==============================] - 1s 398us/step - loss: 0.6154 - acc: 0.7950\n",
            "Epoch 95/200\n",
            "2000/2000 [==============================] - 1s 381us/step - loss: 0.6208 - acc: 0.7970\n",
            "Epoch 96/200\n",
            "2000/2000 [==============================] - 1s 372us/step - loss: 0.6627 - acc: 0.7855\n",
            "Epoch 97/200\n",
            "2000/2000 [==============================] - 1s 358us/step - loss: 0.6052 - acc: 0.8045\n",
            "Epoch 98/200\n",
            "2000/2000 [==============================] - 1s 382us/step - loss: 0.6244 - acc: 0.7970\n",
            "Epoch 99/200\n",
            "2000/2000 [==============================] - 1s 379us/step - loss: 0.5491 - acc: 0.8215\n",
            "Epoch 100/200\n",
            "2000/2000 [==============================] - 1s 375us/step - loss: 0.5559 - acc: 0.8225\n",
            "Epoch 101/200\n",
            "2000/2000 [==============================] - 1s 357us/step - loss: 0.5219 - acc: 0.8360\n",
            "Epoch 102/200\n",
            "2000/2000 [==============================] - 1s 367us/step - loss: 0.5609 - acc: 0.8140\n",
            "Epoch 103/200\n",
            "2000/2000 [==============================] - 1s 371us/step - loss: 0.5078 - acc: 0.8425\n",
            "Epoch 104/200\n",
            "2000/2000 [==============================] - 1s 354us/step - loss: 0.4775 - acc: 0.8465\n",
            "Epoch 105/200\n",
            "2000/2000 [==============================] - 1s 359us/step - loss: 0.5130 - acc: 0.8410\n",
            "Epoch 106/200\n",
            "2000/2000 [==============================] - 1s 360us/step - loss: 0.4725 - acc: 0.8410\n",
            "Epoch 107/200\n",
            "2000/2000 [==============================] - 1s 359us/step - loss: 0.4949 - acc: 0.8455\n",
            "Epoch 108/200\n",
            "2000/2000 [==============================] - 1s 348us/step - loss: 0.4704 - acc: 0.8380\n",
            "Epoch 109/200\n",
            "2000/2000 [==============================] - 1s 362us/step - loss: 0.4454 - acc: 0.8570\n",
            "Epoch 110/200\n",
            "2000/2000 [==============================] - 1s 353us/step - loss: 0.4376 - acc: 0.8580\n",
            "Epoch 111/200\n",
            "2000/2000 [==============================] - 1s 357us/step - loss: 0.4207 - acc: 0.8565\n",
            "Epoch 112/200\n",
            "2000/2000 [==============================] - 1s 359us/step - loss: 0.3845 - acc: 0.8740\n",
            "Epoch 113/200\n",
            "2000/2000 [==============================] - 1s 367us/step - loss: 0.3960 - acc: 0.8785\n",
            "Epoch 114/200\n",
            "2000/2000 [==============================] - 1s 374us/step - loss: 0.4022 - acc: 0.8695\n",
            "Epoch 115/200\n",
            "2000/2000 [==============================] - 1s 358us/step - loss: 0.3927 - acc: 0.8740\n",
            "Epoch 116/200\n",
            "2000/2000 [==============================] - 1s 364us/step - loss: 0.3788 - acc: 0.8740\n",
            "Epoch 117/200\n",
            "2000/2000 [==============================] - 1s 373us/step - loss: 0.3883 - acc: 0.8730\n",
            "Epoch 118/200\n",
            "2000/2000 [==============================] - 1s 358us/step - loss: 0.3966 - acc: 0.8690\n",
            "Epoch 119/200\n",
            "2000/2000 [==============================] - 1s 366us/step - loss: 0.3263 - acc: 0.8910\n",
            "Epoch 120/200\n",
            "2000/2000 [==============================] - 1s 366us/step - loss: 0.3400 - acc: 0.9005\n",
            "Epoch 121/200\n",
            "2000/2000 [==============================] - 1s 379us/step - loss: 0.3383 - acc: 0.8920\n",
            "Epoch 122/200\n",
            "2000/2000 [==============================] - 1s 357us/step - loss: 0.3198 - acc: 0.8955\n",
            "Epoch 123/200\n",
            "2000/2000 [==============================] - 1s 363us/step - loss: 0.3124 - acc: 0.8985\n",
            "Epoch 124/200\n",
            "2000/2000 [==============================] - 1s 375us/step - loss: 0.3293 - acc: 0.8915\n",
            "Epoch 125/200\n",
            "2000/2000 [==============================] - 1s 355us/step - loss: 0.3240 - acc: 0.8935\n",
            "Epoch 126/200\n",
            "2000/2000 [==============================] - 1s 357us/step - loss: 0.2899 - acc: 0.9165\n",
            "Epoch 127/200\n",
            "2000/2000 [==============================] - 1s 358us/step - loss: 0.2851 - acc: 0.9085\n",
            "Epoch 128/200\n",
            "2000/2000 [==============================] - 1s 355us/step - loss: 0.2898 - acc: 0.9040\n",
            "Epoch 129/200\n",
            "2000/2000 [==============================] - 1s 357us/step - loss: 0.2643 - acc: 0.9220\n",
            "Epoch 130/200\n",
            "2000/2000 [==============================] - 1s 358us/step - loss: 0.2759 - acc: 0.9130\n",
            "Epoch 131/200\n",
            "2000/2000 [==============================] - 1s 355us/step - loss: 0.2747 - acc: 0.9110\n",
            "Epoch 132/200\n",
            "2000/2000 [==============================] - 1s 351us/step - loss: 0.2436 - acc: 0.9255\n",
            "Epoch 133/200\n",
            "2000/2000 [==============================] - 1s 363us/step - loss: 0.2402 - acc: 0.9250\n",
            "Epoch 134/200\n",
            "2000/2000 [==============================] - 1s 357us/step - loss: 0.2753 - acc: 0.9090\n",
            "Epoch 135/200\n",
            "2000/2000 [==============================] - 1s 353us/step - loss: 0.2516 - acc: 0.9160\n",
            "Epoch 136/200\n",
            "2000/2000 [==============================] - 1s 359us/step - loss: 0.2367 - acc: 0.9270\n",
            "Epoch 137/200\n",
            "2000/2000 [==============================] - 1s 353us/step - loss: 0.2446 - acc: 0.9240\n",
            "Epoch 138/200\n",
            "2000/2000 [==============================] - 1s 360us/step - loss: 0.2188 - acc: 0.9310\n",
            "Epoch 139/200\n",
            "2000/2000 [==============================] - 1s 365us/step - loss: 0.2118 - acc: 0.9305\n",
            "Epoch 140/200\n",
            "2000/2000 [==============================] - 1s 356us/step - loss: 0.2043 - acc: 0.9405\n",
            "Epoch 141/200\n",
            "2000/2000 [==============================] - 1s 353us/step - loss: 0.1892 - acc: 0.9445\n",
            "Epoch 142/200\n",
            "2000/2000 [==============================] - 1s 358us/step - loss: 0.2022 - acc: 0.9430\n",
            "Epoch 143/200\n",
            "2000/2000 [==============================] - 1s 370us/step - loss: 0.1978 - acc: 0.9385\n",
            "Epoch 144/200\n",
            "2000/2000 [==============================] - 1s 364us/step - loss: 0.1881 - acc: 0.9410\n",
            "Epoch 145/200\n",
            "2000/2000 [==============================] - 1s 358us/step - loss: 0.2027 - acc: 0.9390\n",
            "Epoch 146/200\n",
            "2000/2000 [==============================] - 1s 364us/step - loss: 0.1996 - acc: 0.9400\n",
            "Epoch 147/200\n",
            "2000/2000 [==============================] - 1s 384us/step - loss: 0.1852 - acc: 0.9465\n",
            "Epoch 148/200\n",
            "2000/2000 [==============================] - 1s 356us/step - loss: 0.1761 - acc: 0.9515\n",
            "Epoch 149/200\n",
            "2000/2000 [==============================] - 1s 365us/step - loss: 0.2052 - acc: 0.9360\n",
            "Epoch 150/200\n",
            "2000/2000 [==============================] - 1s 373us/step - loss: 0.1676 - acc: 0.9465\n",
            "Epoch 151/200\n",
            "2000/2000 [==============================] - 1s 393us/step - loss: 0.1783 - acc: 0.9455\n",
            "Epoch 152/200\n",
            "2000/2000 [==============================] - 1s 402us/step - loss: 0.1652 - acc: 0.9505\n",
            "Epoch 153/200\n",
            "2000/2000 [==============================] - 1s 363us/step - loss: 0.1505 - acc: 0.9570\n",
            "Epoch 154/200\n",
            "2000/2000 [==============================] - 1s 386us/step - loss: 0.1573 - acc: 0.9550\n",
            "Epoch 155/200\n",
            "2000/2000 [==============================] - 1s 357us/step - loss: 0.1730 - acc: 0.9505\n",
            "Epoch 156/200\n",
            "2000/2000 [==============================] - 1s 356us/step - loss: 0.1700 - acc: 0.9475\n",
            "Epoch 157/200\n",
            "2000/2000 [==============================] - 1s 355us/step - loss: 0.1401 - acc: 0.9570\n",
            "Epoch 158/200\n",
            "2000/2000 [==============================] - 1s 360us/step - loss: 0.1437 - acc: 0.9565\n",
            "Epoch 159/200\n",
            "2000/2000 [==============================] - 1s 358us/step - loss: 0.1515 - acc: 0.9565\n",
            "Epoch 160/200\n",
            "2000/2000 [==============================] - 1s 356us/step - loss: 0.1616 - acc: 0.9530\n",
            "Epoch 161/200\n",
            "2000/2000 [==============================] - 1s 367us/step - loss: 0.1499 - acc: 0.9520\n",
            "Epoch 162/200\n",
            "2000/2000 [==============================] - 1s 368us/step - loss: 0.1427 - acc: 0.9620\n",
            "Epoch 163/200\n",
            "2000/2000 [==============================] - 1s 378us/step - loss: 0.1395 - acc: 0.9585\n",
            "Epoch 164/200\n",
            "2000/2000 [==============================] - 1s 358us/step - loss: 0.1180 - acc: 0.9645\n",
            "Epoch 165/200\n",
            "2000/2000 [==============================] - 1s 374us/step - loss: 0.1273 - acc: 0.9615\n",
            "Epoch 166/200\n",
            "2000/2000 [==============================] - 1s 367us/step - loss: 0.1292 - acc: 0.9550\n",
            "Epoch 167/200\n",
            "2000/2000 [==============================] - 1s 354us/step - loss: 0.1222 - acc: 0.9625\n",
            "Epoch 168/200\n",
            "2000/2000 [==============================] - 1s 373us/step - loss: 0.1281 - acc: 0.9620\n",
            "Epoch 169/200\n",
            "2000/2000 [==============================] - 1s 365us/step - loss: 0.1216 - acc: 0.9650\n",
            "Epoch 170/200\n",
            "2000/2000 [==============================] - 1s 380us/step - loss: 0.1261 - acc: 0.9625\n",
            "Epoch 171/200\n",
            "2000/2000 [==============================] - 1s 360us/step - loss: 0.1375 - acc: 0.9585\n",
            "Epoch 172/200\n",
            "2000/2000 [==============================] - 1s 388us/step - loss: 0.1032 - acc: 0.9710\n",
            "Epoch 173/200\n",
            "2000/2000 [==============================] - 1s 387us/step - loss: 0.1121 - acc: 0.9670\n",
            "Epoch 174/200\n",
            "2000/2000 [==============================] - 1s 383us/step - loss: 0.1139 - acc: 0.9655\n",
            "Epoch 175/200\n",
            "2000/2000 [==============================] - 1s 356us/step - loss: 0.1120 - acc: 0.9665\n",
            "Epoch 176/200\n",
            "2000/2000 [==============================] - 1s 358us/step - loss: 0.1196 - acc: 0.9575\n",
            "Epoch 177/200\n",
            "2000/2000 [==============================] - 1s 363us/step - loss: 0.1177 - acc: 0.9635\n",
            "Epoch 178/200\n",
            "2000/2000 [==============================] - 1s 372us/step - loss: 0.0980 - acc: 0.9740\n",
            "Epoch 179/200\n",
            "2000/2000 [==============================] - 1s 358us/step - loss: 0.1111 - acc: 0.9665\n",
            "Epoch 180/200\n",
            "2000/2000 [==============================] - 1s 373us/step - loss: 0.0985 - acc: 0.9720\n",
            "Epoch 181/200\n",
            "2000/2000 [==============================] - 1s 367us/step - loss: 0.0920 - acc: 0.9760\n",
            "Epoch 182/200\n",
            "2000/2000 [==============================] - 1s 370us/step - loss: 0.0920 - acc: 0.9710\n",
            "Epoch 183/200\n",
            "2000/2000 [==============================] - 1s 354us/step - loss: 0.0946 - acc: 0.9740\n",
            "Epoch 184/200\n",
            "2000/2000 [==============================] - 1s 383us/step - loss: 0.0966 - acc: 0.9705\n",
            "Epoch 185/200\n",
            "2000/2000 [==============================] - 1s 360us/step - loss: 0.0990 - acc: 0.9690\n",
            "Epoch 186/200\n",
            "2000/2000 [==============================] - 1s 360us/step - loss: 0.0956 - acc: 0.9725\n",
            "Epoch 187/200\n",
            "2000/2000 [==============================] - 1s 354us/step - loss: 0.0884 - acc: 0.9760\n",
            "Epoch 188/200\n",
            "2000/2000 [==============================] - 1s 352us/step - loss: 0.0837 - acc: 0.9795\n",
            "Epoch 189/200\n",
            "2000/2000 [==============================] - 1s 364us/step - loss: 0.0862 - acc: 0.9720\n",
            "Epoch 190/200\n",
            "2000/2000 [==============================] - 1s 354us/step - loss: 0.0833 - acc: 0.9775\n",
            "Epoch 191/200\n",
            "2000/2000 [==============================] - 1s 362us/step - loss: 0.0722 - acc: 0.9800\n",
            "Epoch 192/200\n",
            "2000/2000 [==============================] - 1s 356us/step - loss: 0.0878 - acc: 0.9720\n",
            "Epoch 193/200\n",
            "2000/2000 [==============================] - 1s 374us/step - loss: 0.0838 - acc: 0.9735\n",
            "Epoch 194/200\n",
            "2000/2000 [==============================] - 1s 368us/step - loss: 0.0809 - acc: 0.9785\n",
            "Epoch 195/200\n",
            "2000/2000 [==============================] - 1s 371us/step - loss: 0.0800 - acc: 0.9785\n",
            "Epoch 196/200\n",
            "2000/2000 [==============================] - 1s 362us/step - loss: 0.0853 - acc: 0.9755\n",
            "Epoch 197/200\n",
            "2000/2000 [==============================] - 1s 378us/step - loss: 0.0911 - acc: 0.9735\n",
            "Epoch 198/200\n",
            "2000/2000 [==============================] - 1s 359us/step - loss: 0.0750 - acc: 0.9785\n",
            "Epoch 199/200\n",
            "2000/2000 [==============================] - 1s 367us/step - loss: 0.0834 - acc: 0.9740\n",
            "Epoch 200/200\n",
            "2000/2000 [==============================] - 1s 370us/step - loss: 0.0865 - acc: 0.9755\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "hm9ZQrMp6ndB"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluate"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aXnmFiWUzseg",
        "outputId": "3e79f4dc-3cd7-4ad9-8440-e23a4b1ede3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "evaluation = model.evaluate(X_test, Y_test, batch_size=256, verbose=1)\n",
        "print('Summary: Loss over the test dataset: %.2f, Accuracy: %.2f' % (evaluation[0], evaluation[1]))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 0s 104us/step\n",
            "Summary: Loss over the test dataset: 0.95, Accuracy: 0.80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XdA_fHMxBJ9x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}