{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raden Rizky Falih P\n",
    "\n",
    "# 1301154211"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Classification with scikit-learn: Goole Street View House Numbers\n",
    "\n",
    "By [Ellie Birbeck](https://elliebirbeck.com) for [Hyperion Dev](https://blog.hyperiondev.com/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "For this tutorial we’ll be using a dataset from [Stanford University](http://ufldl.stanford.edu/housenumbers). It contains images of house numbers taken from Google Street View. Each one has been cropped to 32x32 pixels in size, focussing on just the number. Some examples are shown below. \n",
    "\n",
    "![house-numbers](http://ufldl.stanford.edu/housenumbers/32x32eg.png)\n",
    "\n",
    "There are a total of 531131 images in our dataset, and we will load them in as one 4D-matrix of shape `32 x 32 x 3 x 531131`. \n",
    "This represents each 32x32 image in RGB format (so the 3 red, green, blue colour channels) for each of our 531131 images. \n",
    "We’ll be predicting the number shown in the image, from one of ten classes (0-9). Note that in this dataset the number 0 is represented by the label 10. \n",
    "\n",
    "The labels are stored in a 1D-matrix of shape `531131 x 1`. You can check the dimensions of a matrix `X` at any time in your program using `X.shape`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Processing\n",
    "\n",
    "Now let’s begin! To understand the data we’re using, we can start by loading and viewing the image files. First we need to import three libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can load the training dataset into a temporary variable `train_data`, which is a dictionary object. The dictionary contains two variables `X` and `y`. `X` is our 4D-matrix of images, and `y` a 1D-matrix of the corresponding labels. So to access the i-th image in our dataset we would be looking for `X[:,:,:,i]`, and its label would be `y[i]`. Let’s do this for image 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load our dataset\n",
    "train_data = scipy.io.loadmat('datasets/extra_32x32.mat')\n",
    "#train_data = scipy.io.loadmat('train_32x32.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract the images and labels from the dictionary object\n",
    "X = train_data['X']\n",
    "y = train_data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nouAXIlMU/EJkyv8BSCVZbGufacAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26bf1dedbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# view an image (e.g. 10) and print its corresponding label\n",
    "img_index = 11\n",
    "plt.imshow(X[:,:,:,img_index])\n",
    "print(y[img_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The first code block will take a short while to load if you are using the largest dataset. If you are getting errors here, check two things. Firstly, ensure that your downloaded dataset is saved in the same directory as this notebook. And secondly, ensure the filename matches the code (if you downloaded the smaller dataset, you will have a different filename of `'train_32x32.mat'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As you can see, we load up an image showing house number 2, and the console output from our printed label is also 2. You can change the index of the image (to any number between 0 and 531130) and check out different images and their labels if you like. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to use these images with a machine learning algorithm, we first need to vectorise them. \n",
    "\n",
    "This essentially involves stacking up the 3 dimensions of each image (the width x height x colour channels) to transform it into a 1D-matrix. \n",
    "\n",
    "This gives us our feature vector, although it’s worth noting that this is not really a feature vector in the usual sense. \n",
    "\n",
    "Features usually refer to some kind of quantification of a specific trait of the image, not just the raw pixels. \n",
    "\n",
    "Raw pixels can be used successfully in machine learning algorithms, but this is typically with more complex models such as convolutional neural networks, which can learn specific features themselves within their network of layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X = X.reshape(X.shape[0]*X.shape[1]*X.shape[2],X.shape[3]).T\n",
    "y = y.reshape(y.shape[0],)\n",
    "X, y = shuffle(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re also shuffling our data just to be sure there are no underlying distributions. The library we’ve used for this ensures that the index pairings between our images in `X` and their labels in `y` are maintained through the shuffling process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Algorithms\n",
    "\n",
    "Now that we have our feature vector `X` ready to go, we need to decide which machine learning algorithm to use. We don’t need to explicitly program an algorithm ourselves - luckily frameworks like scikit-learn do this for us. \n",
    "\n",
    "If you don’t have any prior experience in machine learning, you can use this helpful [cheat sheet](http://scikit-learn.org/stable/tutorial/machine_learning_map/) to guide you in which algorithms to try out depending on your data. \n",
    "\n",
    "For now we will be using a Random Forest approach with default hyperparameters. You can learn more about Random Forests [here](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html ), but in brief they are a construction of multiple decision trees with an output that averages the results of individual trees to prevent fitting too closely to any one tree. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the necessary library and then define our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "print(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print the classifier to the console to see the parameter settings used. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "We’re now ready to train and test our data. But before we do that, we need to split our total collection of images into two sets - one for training and one for testing. You can also add a third set for development/validation, which you can read more about [here](https://en.wikipedia.org/wiki/Training,_test,_and_validation_sets).\n",
    "\n",
    "Keeping the testing set completely separate from the training set is important, because we need to be sure that the model will perform well in the real world. Once trained, it will have seen many example images of house numbers. We want to be sure that when presented with new images of numbers it hasn’t seen before, that it has actually learnt something from the training and can generalise that knowledge - not just remember the exact images it has already seen.\n",
    "\n",
    "Usually we use between 70-90% of the data for training, though this varies depending on the amount of data collected, and the type of model trained. For example, neural networks are often used with extremely large amounts of data and may sample 99% of the data for training. In this tutorial we’ll go with 80%.  \n",
    "\n",
    "Again, using the largest dataset means that fitting the model may take a little while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Results\n",
    "\n",
    "Now we’re ready to use our trained model to make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.759345552449\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your accuracy result should be ~76%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our model has learnt how to classify house numbers from Google Street View with a relatively high accuracy simply by showing it a few hundred thousand examples. Given a baseline measure of 10% accuracy for random guessing, we’ve made significant progress. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: \n",
    "\n",
    "1. Try to do hyperparameter tuning for Random Forest Classifier then re-train the model. Give explanations for the test results\n",
    "\n",
    "2. Use another classifier and compare the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Change hyperparamater and process in random forest \n",
    "\n",
    "Change hyperparameter (n_estimators=12) and remove random_state (become none) when split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=12, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=12)\n",
    "\n",
    "print(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=12, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.774096980994\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy was increased become 0.774096980994. From this point, it means if we make any changes in parameters n_estimators (increase it) it can increase the accuracy for this model. Why? Because if we change the value of n_estimators to become more higher than before, then it make the trees in the forrest become more numerous as well. The more trees in the forest, it can make more decision/probability to predict (i think). Another reason, because we don't use a random_state so the data maybe become more structural than before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using Another Classifier (Decision Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "clf2 = tree.DecisionTreeClassifier()\n",
    "\n",
    "print(clf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.633944289117\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "preds = clf2.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy for Decision Tree was 0.633944289117. It lower than Random Forest. From this point, it means Random Forest was better than Decision Tree for classify some images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raden Rizky Falih P\n",
    "\n",
    "# 1301154211"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
